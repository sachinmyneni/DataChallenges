{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_ident = pd.read_csv('data/train_identity.csv.gz')\n",
    "df_train_transact = pd.read_csv('data/train_transaction.csv.gz')\n",
    "df_test_ident = pd.read_csv('data/test_identity.csv.gz')\n",
    "df_test_transact = pd.read_csv('data/test_transaction.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.merge(df_train_transact, df_train_ident, on='TransactionID', how='left')\n",
    "df_test = pd.merge(df_test_transact, df_test_ident, on='TransactionID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('df_train_identity.pickle','wb') as f:\n",
    "    pickle.dump(df_train_ident,f,pickle.HIGHEST_PROTOCOL)\n",
    "with open('df_test_identity.pickle','wb') as g:\n",
    "    pickle.dump(df_test_ident,g,pickle.HIGHEST_PROTOCOL)\n",
    "with open('df_train_transactions.pickle','wb') as h:\n",
    "    pickle.dump(df_train_transact,h,pickle.HIGHEST_PROTOCOL)\n",
    "with open('df_test_transactions.pickle','wb') as i:\n",
    "    pickle.dump(df_test_transact,i,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('df_test_leftMerged.pickle','wb') as f:\n",
    "    pickle.dump(df_test,f,pickle.HIGHEST_PROTOCOL)\n",
    "with open('df_train_leftMerged.pickle','wb') as g:\n",
    "    pickle.dump(df_train,g,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all columns which have more than 25% as no-values/NaN\n",
    "miss_val_threshold = 0.25\n",
    "\n",
    "col_to_del = []\n",
    "\n",
    "for c in df_train.columns:\n",
    "    if df_train[c].isnull().sum() > df_train.shape[0]*miss_val_threshold:\n",
    "        col_to_del.append(c)\n",
    "\n",
    "for c in df_test.columns:\n",
    "    if df_train[c].isnull().sum() > df_test.shape[0]*miss_val_threshold:\n",
    "        if c not in col_to_del:\n",
    "            col_to_del.append(c)\n",
    "        \n",
    "col_to_del.append('TransactionID')\n",
    "col_to_del.append('TransactionDT')\n",
    "\n",
    "df_test_transactionids = df_test['TransactionID']\n",
    "\n",
    "df_train.drop(columns=col_to_del, inplace = True)\n",
    "df_test.drop(columns=col_to_del, inplace = True)\n",
    "\n",
    "df_train.fillna(-999, inplace= True)\n",
    "df_test.fillna(-999, inplace= True)\n",
    "\n",
    "col_int16 = ['card1', 'card2', 'card3', 'card5', 'addr1', 'addr2',\n",
    "           'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14',\n",
    "           'D1', 'D10', 'D15', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19',\n",
    "           'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30',\n",
    "           'V31', 'V32', 'V33', 'V34', 'V53', 'V54', 'V55', 'V56', 'V57', 'V58', 'V59',\n",
    "           'V60', 'V61', 'V62', 'V63', 'V64', 'V65', 'V66', 'V67', 'V68', 'V69', \n",
    "           'V70', 'V71', 'V72', 'V73', 'V74', 'V75', 'V76', 'V77', 'V78', 'V79',\n",
    "           'V80', 'V81', 'V82', 'V83', 'V84', 'V85', 'V86', 'V87', 'V88', 'V89',\n",
    "           'V90', 'V91', 'V92', 'V93', 'V94', 'V95', 'V96', 'V97', 'V98', 'V99',\n",
    "           'V100', 'V101', 'V102', 'V103', 'V104', 'V105', 'V106', 'V107', 'V108', 'V109',\n",
    "           'V110', 'V111', 'V112', 'V113', 'V114', 'V115', 'V116', 'V117', 'V118', 'V119',\n",
    "           'V120', 'V121', 'V122', 'V123', 'V124', 'V125', 'V126', 'V127', 'V128', 'V129',\n",
    "           'V130', 'V131', 'V132', 'V133', 'V134', 'V135', 'V136', 'V137', 'V279',\n",
    "           'V280', 'V281', 'V282', 'V283', 'V284', 'V285', 'V286', 'V287', 'V288', 'V289',\n",
    "           'V290', 'V291', 'V292', 'V293', 'V294', 'V295', 'V296', 'V297', 'V298', 'V299',\n",
    "           'V300', 'V301', 'V302', 'V303', 'V304', 'V305', 'V306', 'V307', 'V308', 'V309',\n",
    "           'V310', 'V311', 'V312', 'V313', 'V314', 'V315', 'V316', 'V317', 'V318', 'V319',\n",
    "           'V320', 'V321']\n",
    "\n",
    "for c in col_int16:\n",
    "    df_train[c] = df_train[c].astype(np.int16)\n",
    "    df_test[c] = df_test[c].astype(np.int16)\n",
    "    \n",
    "# one-hot-encoding of categorical values in train dataset\n",
    "col_dummies = ['ProductCD', 'card4', 'card6', 'P_emaildomain']\n",
    "\n",
    "for c in col_dummies:\n",
    "    df_train[c] = pd.get_dummies(df_train[c])\n",
    "    df_test[c] = pd.get_dummies(df_test[c])\n",
    "    \n",
    "# drop one-hot-encoded features     \n",
    "df_train.drop(columns=col_dummies, inplace = True)  \n",
    "df_test.drop(columns=col_dummies, inplace = True) \n",
    "with open('df_test_cleaned.pickle','wb') as f:\n",
    "    pickle.dump(df_test,f,pickle.HIGHEST_PROTOCOL)\n",
    "with open('df_train_cleaned.pickle','wb') as g:\n",
    "    pickle.dump(df_train,g,pickle.HIGHEST_PROTOCOL)\n",
    "with open('df_test_transactionids.pickle','wb') as h:\n",
    "    pickle.dump(df_test_transactionids,h,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('df_train_leftMerged.pickle','rb') as k:\n",
    "    df = pickle.load(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is 22D7-9998\n",
      "\n",
      " Directory of C:\\Users\\myneni\\kaggle\\DataChallenges\\ieee_fraud_detection\n",
      "\n",
      "08/12/2019  06:26 PM    <DIR>          .\n",
      "08/12/2019  06:26 PM    <DIR>          ..\n",
      "08/09/2019  10:18 PM    <DIR>          .ipynb_checkpoints\n",
      "08/06/2019  08:34 PM            61,910 Basic_Classification.ipynb\n",
      "08/12/2019  06:27 AM            38,138 Basic_Classification_2.ipynb\n",
      "08/07/2019  10:44 PM            39,038 Basic_Classification_downsample1.ipynb\n",
      "08/08/2019  09:37 AM            40,514 Basic_Classification_ds_weekdaynum.ipynb\n",
      "08/10/2019  10:28 AM           216,074 Classifier_Comparison.ipynb\n",
      "08/04/2019  07:52 PM    <DIR>          data\n",
      "08/12/2019  06:27 PM       184,437,927 df_test_cleaned.pickle\n",
      "08/12/2019  06:26 PM        39,675,749 df_test_identity.pickle\n",
      "08/12/2019  06:26 PM     1,749,074,963 df_test_leftMerged.pickle\n",
      "08/12/2019  06:27 PM        12,161,233 df_test_transactionids.pickle\n",
      "08/12/2019  06:26 PM     1,581,465,714 df_test_transactions.pickle\n",
      "08/12/2019  06:27 PM       219,683,420 df_train_cleaned.pickle\n",
      "08/12/2019  06:26 PM        40,244,712 df_train_identity.pickle\n",
      "08/12/2019  06:26 PM     2,045,878,050 df_train_leftMerged.pickle\n",
      "08/12/2019  06:26 PM     1,849,578,936 df_train_transactions.pickle\n",
      "08/10/2019  10:27 AM            28,265 ieee-simple-xgboost.ipynb\n",
      "08/08/2019  10:32 PM         3,934,333 kaggle-ieee_fraud_detection.ipynb\n",
      "08/09/2019  10:08 PM            46,712 MLPClassifier.ipynb\n",
      "08/08/2019  11:01 AM             9,181 neuron.ipynb\n",
      "08/12/2019  06:26 PM             7,809 pickle_data.ipynb\n",
      "08/07/2019  10:19 AM             5,632 pickle_dump_example.ipynb\n",
      "08/07/2019  10:22 AM             2,887 pickle_load_example.ipynb\n",
      "08/11/2019  11:26 PM        15,020,489 results.csv\n",
      "08/10/2019  07:50 AM        14,332,264 results_AdaBoost.csv\n",
      "08/10/2019  07:49 AM        14,327,529 results_Decision_Tree.csv\n",
      "08/07/2019  07:25 PM        14,377,896 results_downsample1.csv\n",
      "08/08/2019  12:01 PM        15,232,299 results_downsample1_wkdaynum.csv\n",
      "08/10/2019  07:33 AM        14,376,949 results_Logistic_Regression.csv\n",
      "08/08/2019  12:59 PM        14,421,816 results_mlcp.csv\n",
      "08/10/2019  07:50 AM        15,360,330 results_Naive_Bayes.csv\n",
      "08/10/2019  07:49 AM         9,983,561 results_Nearest_Neighbors.csv\n",
      "08/10/2019  07:49 AM        14,930,313 results_Neural_Net.csv\n",
      "08/10/2019  07:50 AM        14,377,393 results_QDA.csv\n",
      "08/10/2019  07:49 AM        14,319,994 results_Random_Forest.csv\n",
      "08/10/2019  07:51 AM        14,656,197 results_XGBoost.csv\n",
      "08/10/2019  06:38 AM             1,406 score_ranking.txt\n",
      "              35 File(s)  7,912,349,633 bytes\n",
      "               4 Dir(s)  858,255,437,824 bytes free\n"
     ]
    }
   ],
   "source": [
    "!dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [pd.read_csv(f, index_col='TransactionID') \n",
    "       for f in os.listdir('.') if f.startswith('results_') and f.endswith('.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat(dfs,axis=1,join='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506691, 12)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.columns = ['a','b','c','d','e','f','g','h','i','j','k','l']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['isFraud_final'] = df_all[1:12].agg('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.stack().groupby('TransactionID').agg('mean').to_csv('results_mean.csv',header=['isFraud'],index_label='TransactionID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
